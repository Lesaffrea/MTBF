---
title: "MTBF Pulley"
author: "Alain Lesaffre"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---
```{r init, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(janitor)
library(boot)
library(bootstrap)
library(modeest)
library(ggplot2)
library(cowplot)
library(ggeffects)
library(viridis)
library(ggpubr)
library(WVPlots)
library(ggridges)
library(hrbrthemes)
library(gridExtra)
library(SPREDA)
library(tm)
library(comparator)
library(readxl)
library(xgboost)
library(caret) 
library(Ckmeans.1d.dp) # for xgb.ggplot.importance
source("./common.R")
output_data <-janitor::clean_names(read_xlsx(stringr::str_c(here::here(),"/data/Pulley output.table.xlsx")))
```

# Summary

This document analyses the pulley failure, by failure we mean **MTBF**, using distribution instead of the geometric mean.  The issue we face is the low amount of failures for some **floc**, we then check the technical specification of the device.

It appears than we can not build MTBF for those installed devices as we have to much variations in time to failure, the context must be add to the device type, such factor as the contamination factor should be consider as well as the max load apply, we do not have those values in the data set (see ISO 281 for details).

The last part of thios document, we use gradient decsnt to to a prediction on multiclasses on the full dataset, even if we use a hierarchical method the results are pretty interesting. 

# Data set

The data set has about thirty thousand samples, we check mainly, the FLOC, the failure mode and the scheduled date. We do not have the time between failures, we do a grouping of FLO, and material and sort the schedule date and calculate the differences between dates.

Many samples have been with no failure time, those samples have been excluded from the data set. 


```{r build_dataste, echo=FALSE}
data_set <- output_data |> dplyr::group_by(functional_location) |>
                           dplyr::mutate( scheduled_start = as.Date(  scheduled_start,"%d-%m-%Y" , tz ="UTC")) |>
                           dplyr::arrange( scheduled_start, .by_group = TRUE) |>
                           dplyr::mutate( start_time  = dplyr::lag(scheduled_start) ,
                                          failure_time = scheduled_start - start_time ) |>
                           dplyr::filter( !is.na( failure_time) ) |>
                           dplyr::filter( failure_time > 0 ) |>
                           dplyr::mutate( failure_time = as.integer(failure_time),
                                          size_sample = n())
                     
```

The global distribution of time to failure is as follow, we have high gap between failures.

```{r dsp_failure, echo=FALSE}
ggplot( data = data_set) +
  geom_density( aes(x = failure_time), col="orange", fill="orange") +
  theme_minimal() +
  labs( title="Overall time to failure",
        x="Number of days")

```

The distribution of failure is from one to seventy seven per functional location, the low number have to be group per material type and failure mode. 

## Analysing failures

We shall need more attributes per failure, in this case we check the variable description. The distances used:

1. Monge Elkan under construction


```{r distance_description, echo=FALSE}
all_description <-output_data |>
                    dplyr::select(order, description)
all_description <- all_description |>
                      dplyr::mutate( tokens = strsplit(description, '\\s+'))

# distance_monge <-distance(all_description$tokens)
```


# The functional location with one failure 
```{r one_filure, echo=FALSE}
one_failure_data <- data_set |> 
                      dplyr::filter( size_sample== 1)
number_failure_mode <-length(unique(one_failure_data$failure_mode))
```

We have ```r nrow(one_failure_data)``` functional locations with one error , which are distributed over ```r number_failure_mode``` failure modes during the period considered.

The failure mode for those functional areas are as follow, we can see that we have two main type of failures bearing and belt lagging.  The rule of sum is to have thirteen failures to build statistics, we shall select bearing, lagging and shaft.

```{r failure_mode_1, echo=FALSE}
knitr::kable(table(one_failure_data$simplified_failure_mode))
```

The distribution for the three failure modes are as follow.

```{r dsp_failure_mode, echo=FALSE}
dsp_bearing <- one_failure_data |>
                        dplyr::filter(simplified_failure_mode == "Bearing Failure" ) |>
                         ggplot(aes(x = failure_time)) +
                         geom_density( fill="orange" , col="orange")+
                         theme_minimal()+
                         labs( Title = "Bearing for FLOC one failure",
                               x= "Days")
                      
dsp_lagging <- one_failure_data |>
                        dplyr::filter(simplified_failure_mode == "Lagging Failure" ) |>
                         ggplot(aes(x = failure_time)) +
                         geom_density( fill="orange" , col="orange")+
                         theme_minimal()+
                         labs( Title = "Lagging for FLOC one failure",
                               x= "Days")
                      
dsp_shaft <- one_failure_data |>
                        dplyr::filter(simplified_failure_mode == "Shaft Failure" ) |>
                         ggplot(aes(x = failure_time)) +
                         geom_density( fill="orange" , col="orange")+
                         theme_minimal()+
                         labs( Title = "Shaft Failure for FLOC one failure",
                               x= "Days")
                      
plot_grid(dsp_bearing , dsp_lagging, dsp_shaft, 
          labels =c("Bearing","Lagging", "Shaft"),
             ncol = 2, nrow = 2)


bearing_data_set <-one_failure_data |>
                        dplyr::filter(simplified_failure_mode == "Bearing Failure" )
```

We have a big spread for the three types of failure, The main challenge will be how do we want to use the analysis. Taking the bearing as example the model is in the bin between zero and two hundred days. What is surprising is failure less than 500 days for bearing, in this case we have about 25 failures or 10% of the total bearing failures. This short time could be justify if we have bearing with a certain number of days, as mentioned by **SKF** bearing have a long life time and do not failed so often. 

```{r bearing_200, echo=FALSE}
less_200 <-bearing_data_set |> dplyr::filter( failure_time < 200)
hist(less_200$failure_time, col="blue", main="Bearing failures less than 200 days", xlab="Number days")
```

## Check with material description

We work with  material description for **floc** with one failure, The material description defines the technical specification of the device, it is a category. We show the distribution of the mean time based on the material type, we ca notice the spread on the time as well as a deceasing exponential, with most failing before 500 days. 

```{r one_meterial, echo=FALSE}
bearing_material <-bearing_data_set  |> 
                      dplyr::ungroup() |>
                      dplyr::group_by( material_description ) |>
                      dplyr::mutate( number_devices = n()) |>
                      dplyr::summarise( mean_time = mean(failure_time),
                                        median_time = median(failure_time),
                                        number_device = mean(number_devices))
hist(bearing_material$median_time, col="blue", main="Time to failure by material type for bearing", xlab="Time")
```

The low time between failure, which is between zero and and less than 50 mean and median are as follow. We have 13 devices with a time to failure less than 50 days, 

```{r materail_def, echo=FALSE}
knitr::kable(bearing_material$material_description[ bearing_material$mean_time <50])
```

With this data set we have a MTBF of about ```r round(mean(bearing_material$mean_time[bearing_material$mean_time <50]),2)```, but standard deviation of ```r round(sd(bearing_material$mean_time[bearing_material$mean_time <50]),2)```, which show that for the duration less than 50 days we can not deliver reliable MTBF. The sample consider is of thirteen samples *acceptable size for statistics*.

We analyse the devices with a failures greater or equal to twenty and less than fifty days, as we have gap between eight and twenty days in previous sample, we should not consider the bearing for those small values. 

```{r baering_20_50, echo=FALSE}
bearing_20_50 <-bearing_material[bearing_material$mean_time >= 20  & bearing_material$mean_time < 50,]
knitr::kable( bearing_20_50[,c("material_description", "mean_time")])
```

The distribution for those devices is as follow, the distribution is quite unusual, but we use kernel here, the other point we deal with intermittent data, therefore a type such as **Cruston** will define the distribution and able to generate of forecast, the forecast will predict on the whole but not for a particular device. 
We still have a standard deviation of eight, which mean the lower extreme is as 12 days, which make inspection quite often. If the failure was recurring if will be perhaps better to change bearings. 

```{r dsp20_50, echo=FALSE, warning=FALSE}
sd_20_50 <-sd(bearing_20_50$mean_time)
mode_20_50 <-multimode::locmodes(bearing_20_50$mean_time)[1]
low_bound  <- as.numeric(mode_20_50) - 1.96 *sd_20_50

ggplot( data= bearing_20_50) +
  geom_density(aes(x= mean_time), col="orange")+
  theme_minimal()+
  labs( title="For devices between 20 and 50 days")
```

## High failure time for one sample

The same but for device with mean time greater than 4000 days, those material number are unique over all the the bearing data set. 

```{r dsphigh, echo=FALSE}
knitr::kable(bearing_material[ bearing_material$mean_time > 4000, c("material_description", "mean_time")])
check_size <- bearing_data_set |>
                  dplyr::filter( material_description %in%  bearing_material$material_description[bearing_material$mean_time > 4000])

```

The pulley category as follow, we do not have *snub* or *bend* in this sample. 

```{r pulley_cat, echo=FALSE}
knitr::kable(table(check_size$pulley_category))
```


## Analysis more than one device 

```{r more_one, echo=FALSE}
material_data_set <- output_data |> dplyr::group_by(functional_location, material_description) |>
                           dplyr::mutate( scheduled_start = as.Date(  scheduled_start,"%d-%m-%Y" , tz ="UTC")) |>
                           dplyr::arrange( scheduled_start, .by_group = TRUE) |>
                           dplyr::mutate( start_time  = dplyr::lag(scheduled_start) ,
                                          failure_time = scheduled_start - start_time ) |>
                           dplyr::filter( !is.na( failure_time) ) |>
                           dplyr::filter( failure_time > 0 ) |>
                           dplyr::mutate( failure_time = as.integer(failure_time),
                                          size_sample = n())


# keep not nedeed
more_one_device <-bearing_material  |>
                        dplyr::filter( number_device > 1) |>
                        dplyr::mutate( delta = median_time - mean_time)

example_device <-material_data_set[material_data_set$material_description == "PULLEY,BEND;C/W BRGS;C001/3/7",]
example_device <-example_device |>
                      dplyr::mutate( bin_time = cut(failure_time, breaks=c(0,100,500,2000, 3500), include.lowest = TRUE))
```

In this sample rebuilt with slection of materail description, we have same devices with significant difference in time to failure. It means we have conditions, which will reduce the time to failure in this case. It appears that we have few exceptions with low time to failures, which must be analysed.

What is a surprise is the spread of time to failures, concerning the same material description, as shown below we could have more than 2000 days difference in time to failure for the same material description. 

### Analysis same materail description

As example, for the device **PULLEY,BEND;C/W BRGSC001/3/7** which has a mean of 693 days and median of 2 days, for the whole data set, that is ```r nrow(example_device)``` samples, the histogram shows the distribution.

```{r example_device, echo=FALSE}
hist(example_device$failure_time, col="blue", main="Material PULLEY,BEND;C/W BRGSC001/3/7 time to failure", xlab="Number days")
```

Build four failure categories <100, 100 to 500, 500 to 2000 and more than 2000 days to failure we have have the following. The bulk is between 500 days a and 2000 days which is a a wide range. 

```{r table_exception, echo=FALSE}
knitr::kable(table(example_device$bin_time))
```

### Evaluating one contamination factor

This chapter should be taken with  a pitch of salt, as we try to figure out a contamination factor, ignoring the variation in load. The basic rating life is taken as the maximum in this case 3451 days, we exclude from the data set the bearing with less than 85 days of time between failure.

```{r dsp_failure_2, echo=FALSE}
example_device_85<- example_device$failure_time[example_device$failure_time  >85]
value_device <- sort(example_device_85)
step <- (value_device[length(example_device_85)] - value_device[1]) / (length(example_device_85) -1)
line <-numeric(length(value_device))
line[1] <-value_device[1]
for(index in 2:(length(example_device_85) -1)){
    line[index] <-step * (index-1) + value_device[1]
}
line[length(value_device)]  <-value_device[length(value_device)]
dps_time <- data.frame( time= value_device,
                        ref = line)
dps_time$index <-seq(1, nrow(dps_time), 1)

ggplot( data= dps_time, aes( x=index))+
  geom_line( aes(y= time), col="orange")+
  geom_line( aes(y= ref), col="blue") +
  theme_minimal()+
  labs( title="Time to failure for PULLEY,BEND;C/W BRGS;C001/3/7")
# we claculate the contamination factor 
to_check <-value_device[-length(value_device)]
Beta <-numeric(length(to_check))
for( index in length(to_check):1){
        Beta[index] <-to_check[index] / value_device[length(value_device)]
}
Beta[length(to_check)+1 ] <-1
dps_time$coef <-Beta
```


We calculate the coefficient of contamination as mentioned, which are, those value have to be adjust with time:

```{r coef_dsp, echo=FALSE}
knitr::kable( dps_time |> dplyr::select(-index))
```

This device is across multiple functional location and but same plant, the functional loacation are as follow, we can notice the difference in time to failure inside a functional location, which add a complication. 

```{r dsp_Location, echo=FALSE}
knitr::kable(example_device[, c("functional_location", "failure_time", "pulley_category", "description")])
```

# Considering the full data set 

We make the hypothesis that the environment influence the time between failure, therefore we have a factor to difine to do so we consider the full data set with bins. One challenge is the number of functional location of more than 1500. 

```{r full, echo=FALSE, warning=FALSE}
material_data_set <- material_data_set |> 
         dplyr::ungroup() |>  # Remove grouping on material_description
         dplyr::mutate( bin_time = cut(failure_time, breaks=c(0,100,500,2000, 3500), include.lowest = TRUE)) |>
         dplyr::mutate( bin_num = as.integer(bin_time)) |>
         dplyr::filter(!is.na(bin_num))

# This table is difficult to interpret due to size
contingency_all_material <-xtabs( bin_num ~ pulley_category+ functional_location, data = material_data_set)

xg_data <- material_data_set |> 
                   dplyr::mutate( pulley_category = as.integer(factor(pulley_category)),
                                  functional_location = as.integer(factor(functional_location))) |>
                   dplyr::select(pulley_category, functional_location )
xg_label <-  material_data_set$bin_time

train_xgboost <-xgb.DMatrix(data = as.matrix(xg_data),   label = xg_label)
param   <- list(max_depth = 4, eta = 1,  nthread = 2)
model  <-xgb.train( param, train_xgboost, nrounds = 40 )
verify <-predict(model, newdata = train_xgboost)
#. We use the results of the first model to build a second model
xg_data$full_predict <- verify
xg_data$delta_preduct <- verify - as.integer(xg_label)
train_xgboost <-xgb.DMatrix(data = as.matrix(xg_data),   label = xg_label)
model_2  <-xgb.train( param, train_xgboost, nrounds = 50 )
verify_2 <-predict(model_2, newdata = train_xgboost)
names_var <-names(xg_data)
# The variable of importance
importance_matrix = xgb.importance(feature_names= names_var, model = model_2)
xgb.ggplot.importance(importance_matrix) +
        theme_minimal()
# We recalibrate the second predict
delta_2 <- as.integer(xg_label) - as.numeric(verify_2)
recalibrate_verify_2  <-ifelse( delta_2 < 0 , verify_2 + delta_2, ifelse( delta_2 > 0, verify_2+ delta_2, verify_2))
```

We have a prediction on numaric and therefore we recalibrate, the confusion is a sbelow, we certainly overfir in this case, but we show that we can have a pretty good prediction on class of time to failure. 

```{r cobnfusion_matrix, echo=FALSE}
# We show the results
confusionMatrix(factor(recalibrate_verify_2),
                factor( as.integer(xg_label)),
                mode = "everything")
```
        

## Building an universal model

The prevois chapter does not use a test set, in this section we shall use a training set made of .9 of the initial data set and a test set. We do not manage without rebuilding models as shown below.

```{r final_model, echo=FALSE,warning=FALSE, message=FALSE}
set.seed(998)
inTraining <- createDataPartition(material_data_set$bin_time, p = .9, list = FALSE)
training <- material_data_set[ inTraining,]
testing  <- material_data_set[-inTraining,]
# training 
xg_training <- training |> 
                   dplyr::mutate( pulley_category = as.integer(factor(pulley_category)),
                                  functional_location = as.integer(factor(functional_location))) |>
                   dplyr::select(pulley_category, functional_location )

xg_label <-  training$bin_time
train_xgboost <-xgb.DMatrix(data = as.matrix(xg_training),   label = xg_label)
param   <- list(max_depth = 4, eta = .3,  nthread = 2)
model_3  <-xgb.train( param, train_xgboost, nrounds = 50 )
verify   <-predict(model_3, newdata = train_xgboost)
xg_training$full_predict <- verify
xg_training$delta_1      <- verify -1 
xg_training$delta_2      <- verify -2 
xg_training$delta_3      <- verify -3 
xg_training$delta_4      <- verify -4 
train_xgboost_2 <-xgb.DMatrix(data = as.matrix(xg_training),   label = xg_label)
model_4  <-xgb.train( param, train_xgboost_2, nrounds = 50 )
verify_4 <-predict(model_4, newdata = train_xgboost_2)
delta_4 <- as.integer(xg_label) - as.numeric(verify_4)
# just check
recalibrate_verify_4  <-ifelse( delta_4 < 0 , verify_4 + delta_4, ifelse( delta_4 > 0, verify_4+ delta_4, verify_4))
# End of training 

xg_testing <- testing |> 
                   dplyr::mutate( pulley_category = as.integer(factor(pulley_category)),
                                  functional_location = as.integer(factor(functional_location))) |>
                   dplyr::select(pulley_category, functional_location )
xg_label <-  testing$bin_time
testing_xgboost <-xgb.DMatrix(data = as.matrix(xg_testing),   label = xg_label)
testing_predict <-predict(model_3, newdata = testing_xgboost)
xg_testing$full_predict <- testing_predict
xg_testing$delta_1      <- testing_predict -1 
xg_testing$delta_2      <- testing_predict -2 
xg_testing$delta_3      <- testing_predict -3 
xg_testing$delta_4      <- testing_predict -4
testing_xgboost_2 <-xgb.DMatrix(data = as.matrix(xg_testing),   label = xg_label)
testing_predict_2 <-predict(model_4, newdata = testing_xgboost_2)
#'
decimal_predict <- testing_predict_2 - floor(testing_predict_2)
testing_predict_end <-ifelse( decimal_predict  > .5, floor(testing_predict_2)+1 , floor(testing_predict_2))

confusionMatrix(factor(testing_predict_end),
                factor( as.integer(xg_label)),
                mode = "everything")
```







